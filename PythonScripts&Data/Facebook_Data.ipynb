{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSTS fetched in 57.0204529762 sec\n"
     ]
    }
   ],
   "source": [
    "####### Basic python script to fetch the Facebook Data #######\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "\n",
    "Token=\" \",
    "\n",
    "#This URL is encoded with the appropriate fields to be fetched from my Facebook Data\n",
    "URL=\"https://graph.facebook.com/v2.9/me?fields=posts%7Bmessage%2Cid%2Ccreated_time%2Ccomments%7Bmessage%7D%2Clikes%7Bname%7D%2Ctype%2Cstatus_type%7D&access_token=\"+Token\n",
    "FB_file = \"MyFBdata.txt\"\n",
    "\n",
    "resp = requests.get(URL).json()\n",
    "page_resp = copy.copy(resp)\n",
    "\n",
    "#PAGING is necessary as the entire FB data isn't fetched in single go\n",
    "page_info = page_resp[\"posts\"][\"paging\"]\n",
    "\n",
    "t_begin = time.time()\n",
    "\n",
    "#Fetching the FB data from the PAGING Links\n",
    "while \"next\" in page_info and page_info[\"next\"]!=\"\":\n",
    "    #Fetch the NEXT field which holds the URL for the next PAGE\n",
    "    nxt_URL = page_info[\"next\"]\n",
    "    \n",
    "    #Fetch the DATA from the NEXT URL\n",
    "    page_resp = requests.get(nxt_URL).json()\n",
    "    \n",
    "    #Append the POSTS data to the original JSON resp to be written to the output file\n",
    "    nxt_data = page_resp[\"data\"]\n",
    "\n",
    "    for post in nxt_data:\n",
    "        resp[\"posts\"][\"data\"].append(post)\n",
    "    \n",
    "    #Check to see if there is any more Pagination links\n",
    "    if \"paging\" in page_resp:\n",
    "        page_info = page_resp[\"paging\"]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "t_end = time.time()\n",
    "print 'POSTS fetched in {} sec'.format((t_end-t_begin))\n",
    "\n",
    "with open(FB_file, \"w\") as f:\n",
    "    json.dump(resp,f,indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#POSTS:  1076\n",
      "CSV Processing done in 108.435751915 sec\n"
     ]
    }
   ],
   "source": [
    "####### Creating the CSV by analyzing the JSON data #######\n",
    "####### The CSV has COLUMNS namely POST_ID, DATE, TIME, #LIKES, #COMMENTS, #STATUS_TYPE #######\n",
    "\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "\n",
    "FB_data = \"MyFBdata.txt\"\n",
    "FB_CSV = \"FBposts.csv\"\n",
    "Likes = 'LikesComments.txt'\n",
    "\n",
    "POSTS = defaultdict(dict)\n",
    "\n",
    "def updateLikes(likes_data):\n",
    "    global POSTS\n",
    "    for like in likes_data:\n",
    "        try:\n",
    "            name = like['name'].encode('utf-8')\n",
    "            POSTS[\"likes\"][name] +=1\n",
    "        except:\n",
    "            POSTS[\"likes\"][name] =1\n",
    "        \"\"\"\n",
    "        try:\n",
    "            POSTS[\"likes\"][name] +=1\n",
    "        except:\n",
    "            POSTS[\"likes\"][name] =1\n",
    "        \"\"\"   \n",
    "\n",
    "t_begin = time.time()\n",
    "\n",
    "with open(FB_data, \"r\") as FB_json, open(FB_CSV, \"w\") as postfile:\n",
    "    FB_posts = json.load(FB_json)[\"posts\"][\"data\"]\n",
    "    pst_cnt = 0\n",
    "    \n",
    "    #######  Initializing the CSV_WRITER and COLUMNS to be written #########\n",
    "    csv_cols = ['ID','DAY','MONTH','YEAR','DATE','HOUR','MIN','SEC','TIME','TYPE','LIKES','COMMENTS','TOTAL_WORDS']\n",
    "    writer = csv.DictWriter(postfile, fieldnames = csv_cols)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    ####### Iterating through all the posts #######\n",
    "    for post in FB_posts:\n",
    "        csv_dict ={}\n",
    "        post_dict={}\n",
    "        like_count = 0\n",
    "        comments_count = 0\n",
    "        \n",
    "        post_id = str(post[\"id\"])\n",
    "        post_date =''\n",
    "        post_day =''\n",
    "        post_month =''\n",
    "        post_year =''\n",
    "        \n",
    "        post_time =''\n",
    "        post_hour =''\n",
    "        post_min =''\n",
    "        post_sec =''\n",
    "        \n",
    "        ####### Separating DATE & TIME for every post #######\n",
    "        try:\n",
    "            date_time = post[\"created_time\"].split('T')\n",
    "            post_date = date_time[0]\n",
    "            post_year, post_month, post_day = post_date.split('-')\n",
    "            \n",
    "            post_time = date_time[1].split('+')[0]\n",
    "            post_hour, post_min, post_sec = post_time.split(':')\n",
    "        except Exception,e:\n",
    "            print 'Exception in Date-Time: ',str(e)\n",
    "\n",
    "\n",
    "        ####### LIKES for every post #######\n",
    "        if 'likes' in post:\n",
    "            likes_data = post[\"likes\"]   \n",
    "            likes_paging = likes_data[\"paging\"]\n",
    "            like_count += len(likes_data[\"data\"])\n",
    "\n",
    "            ####### Updating the LIKES count for every USER #######\n",
    "            try:\n",
    "                updateLikes(likes_data[\"data\"])\n",
    "\n",
    "                while \"next\" in likes_paging:   \n",
    "                    #print 'HERE:'\n",
    "                    next_like_page = likes_paging[\"next\"]\n",
    "                    #print 'NEXT Link: ',next_like_page  \n",
    "\n",
    "                    page_resp = requests.get(next_like_page).json()\n",
    "\n",
    "                    ####### Updating the LIKES count for every USER #######\n",
    "                    updateLikes(page_resp[\"data\"])\n",
    "\n",
    "                    like_count += len(page_resp[\"data\"])\n",
    "                    likes_paging = page_resp[\"paging\"]\n",
    "            except Exception,e:\n",
    "                print 'Exception in Likes: ',str(e)\n",
    "\n",
    "\n",
    "        ####### COMMENTS for every post #######\n",
    "        if 'comments' in post:\n",
    "            comments_data = post[\"comments\"]\n",
    "            comments_paging = comments_data[\"paging\"]\n",
    "            comments_count += len(comments_data[\"data\"])\n",
    "\n",
    "            ####### Updating the COMMENTS count for every USER #######\n",
    "            try:\n",
    "                while 'next' in comments_paging:\n",
    "                    next_comment_page = comments_paging[\"next\"]\n",
    "                    page_resp = requests.get(next_comment_page).json()\n",
    "\n",
    "                    ####### Updating the COMMENTS count for every USER #######\n",
    "\n",
    "                    comments_count += len(page_resp[\"data\"])\n",
    "                    comments_paging = page_resp[\"paging\"]\n",
    "            except Exception,e:\n",
    "                print 'Exception in Comments: ',str(e)\n",
    "        \n",
    "        word_count = 0 \n",
    "        ####### Count of Words used in the POST #######\n",
    "        if 'message' in post:\n",
    "            post_msg = filter(None,re.split(\"[- \\,:;!\\\\n]\", post[\"message\"].lower()))\n",
    "            word_count = len(post_msg)\n",
    "        \n",
    "        ####### WRITING the ROW to the CSV #######\n",
    "        csv_dict['ID']   = post_id\n",
    "        \n",
    "        csv_dict['DATE'] = post_date\n",
    "        csv_dict['YEAR'] = post_year\n",
    "        csv_dict['MONTH']= post_month\n",
    "        csv_dict['DAY']  = post_day\n",
    "        \n",
    "        csv_dict['TIME'] = post_time\n",
    "        csv_dict['HOUR'] = post_hour\n",
    "        csv_dict['MIN'] = post_min\n",
    "        csv_dict['SEC'] = post_sec\n",
    "        \n",
    "        csv_dict['TYPE'] = post[\"type\"]\n",
    "        csv_dict['LIKES']= like_count\n",
    "        csv_dict['COMMENTS'] = comments_count\n",
    "        \n",
    "        csv_dict['TOTAL_WORDS'] = word_count\n",
    "        pst_cnt+=1\n",
    "\n",
    "        writer.writerow(csv_dict)\n",
    "    \n",
    "    print '#POSTS: ',pst_cnt\n",
    "    \n",
    "t_end = time.time()\n",
    "\n",
    "print 'CSV Processing done in {} sec'.format(t_end-t_begin)\n",
    "\n",
    "####### Writing the POSTS Dict (LIKES & COMMENTS) to a TEXT File #######\n",
    "with open(Likes, 'w') as f:\n",
    "    json.dump(POSTS,f,indent=4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python2.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "#Sentiment Analysis method using VADER#\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import json\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_sentiments(sentence):\n",
    "\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076 records updated with VADER Sentiment Score in 8.46387600899 seconds\n"
     ]
    }
   ],
   "source": [
    "####### Reading the FBposts.csv and appending to each record the SENTIMENT POLARITY of each Post #######\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import csv\n",
    "\n",
    "TXT_INP = \"MyFBdata.txt\"\n",
    "CSV_INP = \"FBposts.csv\"\n",
    "CSV_OUT = \"FBposts_Senti.csv\"\n",
    "\n",
    "t1= time.time()\n",
    "with open(CSV_INP,\"r\") as inp, open(TXT_INP, \"r\") as posts, open(CSV_OUT,\"w\") as out:\n",
    "    posts_data = json.load(posts)[\"posts\"][\"data\"]\n",
    "    \n",
    "    reader = csv.reader(inp)\n",
    "    headers=reader.next()\n",
    "    \n",
    "    rows = list(reader)\n",
    "    \n",
    "    headers.append('POS')\n",
    "    headers.append('NEG')\n",
    "    headers.append('NEU')\n",
    "    headers.append('COMP')\n",
    "    \n",
    "    writer = csv.writer(out)\n",
    "    writer.writerow(headers)\n",
    "    \n",
    "    count = 0\n",
    "    for post in posts_data:\n",
    "        post_id = str(post[\"id\"])\n",
    "        #print post_id\n",
    "        pos=neg=neu=comp=0.0\n",
    "\n",
    "        for row in rows:\n",
    "            if str(row[0])==post_id:\n",
    "                if \"message\" in post and len(post[\"message\"])>=5:\n",
    "                    #print 'Row in CSV: ',row\n",
    "                    senti=analyze_sentiments(post[\"message\"])\n",
    "                    pos=round(senti['pos'],3)\n",
    "                    neg=round(senti['neg'],3)\n",
    "                    neu=round(senti['neu'],3)\n",
    "                    comp=round(senti['compound'],3)\n",
    "                break\n",
    "                \n",
    "        row.append(pos)\n",
    "        row.append(neg)\n",
    "        row.append(neu)\n",
    "        row.append(comp)\n",
    "        \n",
    "        writer.writerow(row)\n",
    "        count+=1\n",
    "\n",
    "t2 = time.time()\n",
    "print '{} records updated with VADER Sentiment Score in {} seconds'.format(count, (t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS and POSTS updated in 0.177491188049 seconds\n",
      "#POSTS with messages:  342\n",
      "Length of resulting DICT:  342\n"
     ]
    }
   ],
   "source": [
    "###### Creating the DICTIONARY of the LEXOGRAPHIC ANALYSIS of the POSTS #######\n",
    "###### Writing the Created DICTIONARY to a CSV #######\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import pprint\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "\n",
    "FB_data = \"MyFBdata.txt\"\n",
    "WORDS = \"words.txt\"\n",
    "MSGS = \"posts.txt\"\n",
    "\n",
    "Messages = defaultdict(list)\n",
    "POST_data = defaultdict(dict)\n",
    "\n",
    "t1 = time.time()\n",
    "with open(FB_data, \"r\") as FB_json:\n",
    "    FB_posts = json.load(FB_json)[\"posts\"][\"data\"]\n",
    "    msg_count = 0\n",
    "    \n",
    "    ####### CHARACTERS to be ignored in the MSGS and STEMMER to be used ########\n",
    "    bad_chars = ['(',')','!','-',',']\n",
    "    \n",
    "    ####### Extracting the STOPWORDS or words which don't imply any specific meaning ####### \n",
    "    stop_wrds = stopwords.words('english')\n",
    "    #print stop_wrds\n",
    "    \n",
    "    ####### Iterating through all the posts #######\n",
    "    for post in FB_posts:\n",
    "        post_dict={}\n",
    "        post_id = str(post[\"id\"])\n",
    "\n",
    "        #Analyze MESSAGES of POSTS and append the analytics to a dictionary\n",
    "        if \"message\" in post:\n",
    "            post_id = str(post[\"id\"])\n",
    "            post_words = defaultdict(int)\n",
    "            hash_tags = defaultdict(list)\n",
    "            \n",
    "            ####### Removing STOPWORDS from the Words written in the POST #######\n",
    "            #The POST message has to be converted to lowercase as the stopwords are in Lowercase only #\n",
    "            \n",
    "            post_msg = filter(None,re.split(\"[- \\,:;!\\\\n]\", post[\"message\"].lower()))\n",
    "            post_msg = list(set(post_msg) - set(stop_wrds))\n",
    "            \n",
    "            if len(post_msg)<5:\n",
    "                continue\n",
    "            \n",
    "            ####### Appending the POST[\"message\"] to the Messages DICT #######\n",
    "            Messages[\"posts\"].append(post[\"message\"].lower())\n",
    "            \n",
    "            ####### RE pattern to look for in the words of the POST Message #######\n",
    "            ptrn=re.compile('[^a-zA-Z]')\n",
    "            for str_word in post_msg:\n",
    "                try:\n",
    "                    word = str(str_word.lower())\n",
    "                    if word[0]=='#':\n",
    "                        hashtags.append(word)\n",
    "                    else:\n",
    "                        # Extracting only the Alphabetic part of that word #\n",
    "                        word = ptrn.sub('',word)\n",
    "                        if word!=\"\":\n",
    "                            #stmword = str(stemmer.stem(word))\n",
    "                            post_words[word]+=1                    \n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            ####### Appending the POST data only when #words>10\n",
    "            if len(post_words)>10:\n",
    "                msg_count+=1\n",
    "                post_dict[\"hash\"] = hash_tags\n",
    "                post_dict[\"words\"] = post_words\n",
    "\n",
    "                ####### Appending analytics of POST to the DICTIONARY #######\n",
    "                POST_data[post_id] = post_dict\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print 'WORDS and POSTS updated in {} seconds'.format(t2-t1)\n",
    "    print '#POSTS with messages: ',msg_count\n",
    "    print 'Length of resulting DICT: ',len(POST_data)\n",
    "\n",
    "####### Dumping the JSON into a TEXT File #######\n",
    "with open(WORDS, \"w\") as words, open(MSGS, \"w\") as msgs:\n",
    "    json.dump(POST_data, words, indent=4)\n",
    "    json.dump(Messages, msgs, indent=4)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076 Records written in 0.0235300064087 seconds\n"
     ]
    }
   ],
   "source": [
    "####### Adding the POSTS Length column to the Initial CSV #######\n",
    "\n",
    "OUT = 'FBposts_Words.csv'\n",
    "WORDS = 'words.txt'\n",
    "\n",
    "t1 = time.time()\n",
    "with open('FBposts_Senti.csv',\"r\") as inp, open(WORDS, \"r\") as words, open(OUT,\"w\") as out:\n",
    "    grammar = json.load(words)\n",
    "    \n",
    "    reader = csv.reader(inp)\n",
    "    headers=reader.next()\n",
    "    headers.append('UNIQ_WORDS')\n",
    "    \n",
    "    writer = csv.writer(out)\n",
    "    writer.writerow(headers)\n",
    "    \n",
    "    count = 0\n",
    "    for row in reader:\n",
    "        post_id = str(row[0])\n",
    "        if post_id in grammar:\n",
    "            words = len(grammar[post_id]['words'])\n",
    "            row.append(words)\n",
    "        else:\n",
    "            row.append(0)\n",
    "            \n",
    "        writer.writerow(row)\n",
    "        count+=1\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print '{} Records written in {} seconds'.format(count, (t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Unique words used all over:  4562\n"
     ]
    }
   ],
   "source": [
    "####### Creating a Dictionary of Total UNIQUE words used  across all the POSTS #######\n",
    "from collections import defaultdict\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "POSTS = 'words.txt'\n",
    "BAG = 'bag.txt'\n",
    "BAG_WORDS = 'Bag_Of_Words.csv'\n",
    "\n",
    "Words_Bag = defaultdict(dict)\n",
    "with open(POSTS, \"r\") as posts, open(BAG,\"w\") as bag, open(BAG_WORDS,\"w\") as bagfile:\n",
    "    words = json.load(posts)\n",
    "    \n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    #######  Initializing the CSV_WRITER and COLUMNS to be written #########\n",
    "    csv_cols = ['STEM','WORDS','NUM_WORDS', 'POSTS', 'COUNT']\n",
    "    writer = csv.DictWriter(bagfile, fieldnames = csv_cols)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for post in words:\n",
    "        vocab=words[post][\"words\"]\n",
    "        for word in vocab:\n",
    "            stem = str(stemmer.stem(word))\n",
    "            \n",
    "            if stem in Words_Bag:\n",
    "                if word not in Words_Bag[stem][\"words\"]:\n",
    "                    Words_Bag[stem][\"words\"].append(word)\n",
    "                Words_Bag[stem][\"count\"] += vocab[word]\n",
    "                Words_Bag[stem][\"posts\"] += 1\n",
    "            else:\n",
    "                Words_Bag[stem][\"words\"] = [word]\n",
    "                Words_Bag[stem][\"count\"] = vocab[word]\n",
    "                Words_Bag[stem][\"posts\"] = 1\n",
    "                \n",
    "    for stem in Words_Bag:\n",
    "        stm = stem\n",
    "        nm_wrds = len(Words_Bag[stem][\"words\"])\n",
    "        words = ', '.join(str(word) for word in Words_Bag[stem][\"words\"])\n",
    "        posts = Words_Bag[stem][\"posts\"]\n",
    "        count = Words_Bag[stem][\"count\"]\n",
    "        \n",
    "        writer.writerow(dict([('STEM',stm),('WORDS',words),('NUM_WORDS',nm_wrds),('POSTS',posts),('COUNT',count)]))\n",
    "        \n",
    "    print 'No. of Unique words used all over: ',len(Words_Bag.keys())\n",
    "    json.dump(Words_Bag,bag,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-40-fae002140317>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-fae002140317>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    WARNING: The below Updating Script is to be used only when there is a danger of Token Expiry,\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "WARNING: The below Updating Script is to be used only when there is a danger of Token Expiry,\n",
    "    so the LIKES nd COMMENTS Links have to be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### Script to update the LIKES & COMMENTS URLs due to TOKEN Expiry #######\n",
    "####### Write the Updated Data to a New Text File #######\n",
    "\"\"\"\n",
    "FB_data = \"MyFBdata.txt\"\n",
    "New_data = 'MyUpdatedFBdata.txt'\n",
    "TOKEN = 'EAAUjAlaG2CkBAOgf8ZCqFNISPIHKDUpfZCrxiJlvo0YrX2Mc5vgKt4lXEJQWFcbiIrK1e5tghgWvz5RFW56GPd9D9rBGZARosZBpJFRLLVR8kfpTN3IK3ums1YP8TJiKN9tZAeXgz8cnGke2uAFEZB4k9LaZBZCWZCuHyvsZB0kcDFN1wQe86bzaq2ZAUGu3QWRPNAZD'\n",
    "\n",
    "with open(FB_data,\"r\") as links, open(New_data, \"w\") as out:\n",
    "    data = json.load(links)\n",
    "    FB_posts = data[\"posts\"][\"data\"]\n",
    "    \n",
    "    for post in FB_posts:\n",
    "        if 'likes' in post and 'next' in post['likes']['paging']:\n",
    "            URL = post['likes']['paging']['next']\n",
    "            front,end = URL.split('access_token')[0],URL.split('access_token')[1]\n",
    "            \n",
    "            tokenless_end = end.split('&fields')[1]\n",
    "            newURL = front + 'access_token=' + TOKEN + '&fields' + tokenless_end\n",
    "            \n",
    "            #print 'NEW_URL : {}'.format(newURL)\n",
    "            post['likes']['paging']['next'] = newURL\n",
    "            #break\n",
    "            \n",
    "        if 'comments' in post and 'next' in post['comments']['paging']:\n",
    "            URL = post['comments']['paging']['next']\n",
    "            front,end = URL.split('access_token')[0],URL.split('access_token')[1]\n",
    "            \n",
    "            tokenless_end = end.split('&fields')[1]\n",
    "            newURL = front + 'access_token=' + TOKEN + '&fields' + tokenless_end\n",
    "            \n",
    "            #print 'NEW_URL : {}'.format(newURL)\n",
    "            post['comments']['paging']['next'] = newURL\n",
    "            #break            \n",
    "        \n",
    "    print 'Modification Done!'\n",
    "    data[\"posts\"][\"data\"] = FB_posts\n",
    "    \n",
    "    json.dump(data,out,indent=4)\n",
    "\"\"\"        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
